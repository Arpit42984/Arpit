{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSvJt7VSp3Xv"
      },
      "outputs": [],
      "source": [
        " !pip install tenseal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m-SpznFVZQm"
      },
      "source": [
        "**Prelims**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Po8zVhsVF0M"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Softmax, Input\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tensorflow.keras.models import load_model,  Model\n",
        "from sklearn.metrics import classification_report\n",
        "import tenseal as ts\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKNsXbHlKPCG"
      },
      "source": [
        "Pre-Trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc--w6PVVXV6"
      },
      "outputs": [],
      "source": [
        "# scale pixels\n",
        "def prep_pixels(train, test):\n",
        "    # convert from integers to floats\n",
        "    train_norm = train.astype('float32')\n",
        "    test_norm = test.astype('float32')\n",
        "    # normalize to range 0-1\n",
        "    train_norm = train_norm / 255.0\n",
        "    test_norm = test_norm / 255.0\n",
        "    # return normalized images\n",
        "    return train_norm, test_norm\n",
        "\n",
        "def load_dataset():\n",
        "    # Load the CIFAR-10 dataset\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    # Initialize empty lists to store selected images and labels\n",
        "    selected_images = []\n",
        "    selected_labels = []\n",
        "    test_images = []\n",
        "    test_labels = []\n",
        "\n",
        "    # Iterate through the first four labels for training data\n",
        "    for label in range(4):\n",
        "        # Get indices of images with the current label\n",
        "        indices = np.where(y_train.squeeze() == label)[0]\n",
        "        selected_indices = indices[4000:5000]  #Since we want a subset of the first 1000 images\n",
        "        selected_images.extend(x_train[selected_indices])\n",
        "        selected_labels.extend(y_train[selected_indices])\n",
        "\n",
        "    # Iterate through the first four labels for test data\n",
        "    for label in range(4):\n",
        "        # Get indices of images with the current label\n",
        "        indices = np.where(y_test.squeeze() == label)[0]\n",
        "        test_images.extend(x_test[indices])\n",
        "        test_labels.extend(y_test[indices])\n",
        "\n",
        "    # Convert the lists to NumPy arrays\n",
        "    selected_images = np.array(selected_images)\n",
        "    selected_labels = np.array(selected_labels)\n",
        "    test_images = np.array(test_images)\n",
        "    test_labels = np.array(test_labels)\n",
        "\n",
        "    # One-hot encode target values\n",
        "    selected_labels = to_categorical(selected_labels, num_classes=4)\n",
        "    test_labels = to_categorical(test_labels, num_classes=4)\n",
        "\n",
        "    return selected_images, selected_labels, test_images, test_labels\n",
        "\n",
        "\n",
        "def define_cifar10_model():\n",
        "    inputs = Input(shape=(32, 32, 3))\n",
        "\n",
        "    x = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(inputs)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(64, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "\n",
        "    x = Dense(4, kernel_initializer='he_uniform')(x)\n",
        "    outputs = Softmax()(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Compile model\n",
        "    opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the initial model and save its weights\n",
        "initial_model = define_cifar10_model()\n",
        "initial_weights = initial_model.get_weights()\n",
        "\n",
        "# Function to create a new model with the same initial weights\n",
        "def create_model_with_initial_weights():\n",
        "    model = define_cifar10_model()\n",
        "    model.set_weights(initial_weights)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Run the test harness for evaluating the Pre-Trained model\n",
        "def run_cifar10_test_harness():\n",
        "    # Load CIFAR-10 dataset\n",
        "    trainX, trainY, testX, testY = load_dataset()\n",
        "    trainX, testX = prep_pixels(trainX, testX)\n",
        "    # Define model\n",
        "    model = create_model_with_initial_weights()\n",
        "    # Fit model\n",
        "    history = model.fit(trainX, trainY, epochs=50, batch_size=64, validation_data=(testX, testY), verbose=1)\n",
        "    # Save model\n",
        "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
        "    print('> %.3f' % (acc * 100.0))\n",
        "    # learning curves\n",
        "    model.save('cifar10.keras')\n",
        "\n",
        "# Entry point, run the CIFAR-10 test harness\n",
        "run_cifar10_test_harness()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model for extracting feature vectors from Penultimate layer"
      ],
      "metadata": {
        "id": "OFd2rPOyxUqE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qWQYq5AUuod"
      },
      "outputs": [],
      "source": [
        "original_model = load_model('cifar10.keras')\n",
        "original_model.summary()\n",
        "layer_before_softmax_output = original_model.layers[-2].output\n",
        "layer_before_softmax_model = Model(inputs=original_model.input, outputs=layer_before_softmax_output)\n",
        "layer_before_softmax_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yimE56JdKWYv"
      },
      "source": [
        "Dataset Subset selector and feature extractor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e9dd12dBWJJ4"
      },
      "outputs": [],
      "source": [
        "#This function takes number of data samples for each class as arguments and returns dataset with same number of datasamples\n",
        "def subset_cifar10(args):\n",
        "    if len(args) != 4:\n",
        "        raise ValueError(\"Exactly 4 integer arguments are required for the first four classes.\")\n",
        "\n",
        "    # Load CIFAR-10 dataset\n",
        "    (x_train, y_train), _ = cifar10.load_data()\n",
        "\n",
        "    # Convert y_train to a 1D array\n",
        "    y_train = y_train.squeeze()\n",
        "\n",
        "    # Count occurrences of each class\n",
        "    class_counts = defaultdict(int)\n",
        "    for label in y_train:\n",
        "        class_counts[label] += 1\n",
        "\n",
        "    # Create a dictionary to store indices of samples for each class\n",
        "    class_indices = defaultdict(list)\n",
        "    for i, label in enumerate(y_train):\n",
        "        class_indices[label].append(i)\n",
        "\n",
        "    # Select the subset of samples for the first four classes\n",
        "    subset_indices = []\n",
        "    for label in range(4):\n",
        "        num_samples = args[label]\n",
        "        if num_samples > class_counts[label]:\n",
        "            raise ValueError(f\"Requested {num_samples} samples for class {label}, but there are only {class_counts[label]} samples available.\")\n",
        "        subset_indices.extend(class_indices[label][:num_samples])\n",
        "\n",
        "    # Shuffle the indices to mix classes\n",
        "    np.random.shuffle(subset_indices)\n",
        "\n",
        "    # Extract the subset of images and labels\n",
        "    x_subset = x_train[subset_indices]\n",
        "    y_subset = y_train[subset_indices]\n",
        "\n",
        "    return x_subset, y_subset\n",
        "\n",
        "def feature_extractor(images,model = original_model):\n",
        "    images,_ = prep_pixels(images,images)\n",
        "\n",
        "    feature_vectors = layer_before_softmax_model.predict(images, verbose=0).reshape(len(images), -1)\n",
        "\n",
        "    row_norms = np.linalg.norm(feature_vectors , axis=1, keepdims=True)\n",
        "\n",
        "    normalized_feature_vectors = feature_vectors / row_norms\n",
        "\n",
        "    return normalized_feature_vectors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jfjVi--Kdq1"
      },
      "source": [
        "**BASELINE-1** Under sampling at each client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CR-VmFzyzz1p"
      },
      "outputs": [],
      "source": [
        "# Under-sampling for baseline\n",
        "\n",
        "def count_zeros(number):\n",
        "    if number <= 0 or number >= 1:\n",
        "        raise ValueError(\"Number should be between 0 and 1, exclusive.\")\n",
        "\n",
        "    count = 0\n",
        "    while number < 1:\n",
        "        number *= 10\n",
        "        count += 1\n",
        "        if number >= 1:\n",
        "            break\n",
        "\n",
        "    return count - 1\n",
        "\n",
        "\n",
        "\n",
        "def base_under_sampling(images, labels, LD):\n",
        "  LI = min(LD)/max(LD)\n",
        "  if(LI >= 0.05):\n",
        "    print(\"Client aleady Balanced\")\n",
        "    return  images, labels, LD\n",
        "  x_train = images\n",
        "  y_train = labels\n",
        "  distr = LD\n",
        "  while(LI < 0.05):\n",
        "    selected_class = np.argmax(distr)\n",
        "    num = count_zeros(LI)\n",
        "    class_indices = np.where(y_train == selected_class)[0]\n",
        "    mc_images = x_train[class_indices]\n",
        "    feature_vectors = feature_extractor(mc_images)\n",
        "    cosine_sim_matrix = cosine_similarity(feature_vectors)\n",
        "    n = cosine_sim_matrix.shape[0]\n",
        "    #print(n)\n",
        "\n",
        "    # Compute the mean and variance for each row\n",
        "    row_means = np.mean(cosine_sim_matrix, axis=1)\n",
        "    row_variances = np.var(cosine_sim_matrix, axis=1)\n",
        "\n",
        "    # Sort the rows based on mean values in descending order\n",
        "    sorted_indices = np.argsort(row_means)[::-1]\n",
        "    #print(num+1)\n",
        "    # Select the top 1/10 of the rows with the highest mean values\n",
        "    selected_indices = sorted_indices[:int((10**(num+1))/5)]\n",
        "\n",
        "    # Calculate the variance of the selected rows\n",
        "    selected_var = np.var(cosine_sim_matrix[selected_indices], axis=0)\n",
        "\n",
        "    # Iterate through the remaining rows and select additional rows one by one\n",
        "    for i in sorted_indices[int((10**(num+1))/5):]:\n",
        "        temp_indices = np.append(selected_indices, i)\n",
        "        temp_var = np.var(cosine_sim_matrix[temp_indices], axis=0)\n",
        "        # If variance is lower, add the row to the selection\n",
        "        if np.sum(temp_var) < np.sum(selected_var):\n",
        "            selected_var = temp_var\n",
        "            selected_indices = temp_indices\n",
        "        # Break if the desired number of rows is reached\n",
        "        if len(selected_indices) == int((10**(num+1))/5):\n",
        "            break\n",
        "    #print(\"images to be removed: \",len(selected_indices),\" of class \",(selected_class))\n",
        "    # Convert lists to numpy arrays for easy indexing\n",
        "\n",
        "\n",
        "    # Get indices of the samples belonging to the selected class\n",
        "    selected_class_indices = np.where(y_train == selected_class)[0]\n",
        "\n",
        "    # Translate samples_to_remove indices to indices in the original dataset\n",
        "    remove_indices = selected_class_indices[selected_indices]\n",
        "\n",
        "    # Create masks for removing samples\n",
        "    mask = np.ones(len(y_train), dtype=bool)\n",
        "    mask[remove_indices] = False\n",
        "    # Apply masks to x_train and y_train to remove selected samples\n",
        "    x_train = x_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    new_distr = np.copy(distr)\n",
        "    new_distr[selected_class] -= len(remove_indices)\n",
        "    distr = new_distr\n",
        "    LI = min(distr)/max(distr)\n",
        "    #print(LI)\n",
        "  return x_train, y_train, distr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkI1t1uWKkLg"
      },
      "source": [
        "**Baseline -2** Over Sampling at each client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rNKm7QNfJ6MO"
      },
      "outputs": [],
      "source": [
        "def img_augmentation(images, save_dir, num_augmented_images):\n",
        "  prefix='test',\n",
        "  data_augment = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        "  )\n",
        "\n",
        "  if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "  # Convert images to numpy arrays and stack them\n",
        "  image_arrays = [img_to_array(image) for image in images]\n",
        "  image_array_batch = np.stack(image_arrays)\n",
        "\n",
        "  # Reshape the batch array to add an extra dimension\n",
        "  image_array_batch = image_array_batch.reshape((len(images),) + image_array_batch.shape[1:])\n",
        "\n",
        "  total_images_to_generate = num_augmented_images\n",
        "  i = 0\n",
        "\n",
        "  for batch in data_augment.flow(image_array_batch, batch_size=len(images), save_to_dir=save_dir, save_prefix=prefix, save_format='jpg'):\n",
        "    i += len(batch)\n",
        "    if i >= total_images_to_generate:\n",
        "      break\n",
        "  return 0\n",
        "\n",
        "\n",
        "def over_sampling_from_folder_clien(images,labels, LD):\n",
        "  LI = min(LD)/max(LD)\n",
        "  #print(LI)\n",
        "  if(LI >= 0.05):\n",
        "    print(\"CLient Already Balanced\")\n",
        "    return images,labels, LD\n",
        "  new_images = []\n",
        "  new_labels = []\n",
        "  image_folder = '/content/generated_images3'\n",
        "  existing_images = images\n",
        "  existing_labels = labels\n",
        "  distr = LD\n",
        "  while(LI < 0.05):\n",
        "    selected_class = np.argmin(distr)\n",
        "    class_indices = np.where(existing_labels == selected_class)[0]\n",
        "    selected_images = existing_images[class_indices]\n",
        "    multiple = int(1/(LI))\n",
        "    img_augmentation(selected_images,image_folder ,multiple)\n",
        "    # Count the number of existing images for the specified class label\n",
        "    num_existing_images = len(class_indices)\n",
        "    #print(num_existing_images, end = \" \")\n",
        "    # Calculate the number of images to add\n",
        "    num_images_to_add = num_existing_images\n",
        "    # Collect all filenames in the folder\n",
        "    filenames = os.listdir(image_folder)\n",
        "    # Shuffle the filenames\n",
        "    random.shuffle(filenames)\n",
        "\n",
        "    # Iterate through the images in the folder\n",
        "    for filename in filenames:\n",
        "      filepath = os.path.join(image_folder, filename)\n",
        "      image = cv2.imread(filepath)\n",
        "      new_images.append(image)\n",
        "      new_labels.append(selected_class)\n",
        "    # Convert lists to numpy arrays\n",
        "    np_new_images = np.array(new_images)\n",
        "    np_new_labels = np.array(new_labels)\n",
        "    # Concatenate new images and labels with existing dataset\n",
        "    existing_images = np.concatenate((existing_images, np_new_images), axis=0)\n",
        "    existing_labels = np.concatenate((existing_labels, np_new_labels), axis=0)\n",
        "    # Update distribution\n",
        "    new_distribution = np.copy(distr)\n",
        "    new_distribution[selected_class] += len(new_images)\n",
        "    print(num_images_to_add,\" : images added successfully in class\",selected_class)\n",
        "    distr = new_distribution\n",
        "    LI = min(distr)/max(distr)\n",
        "    #print(LI)\n",
        "    for file in glob.glob(\"/content/generated_images3/*\"):\n",
        "      os.remove(file)\n",
        "  return  existing_images, existing_labels, distr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQYqlXgNTTOV"
      },
      "source": [
        "**MAIN CODE**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "All methods defined are for FLICKER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ICH6lDWCWQNC"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Server:\n",
        "    _instance = None  # Class variable to store the single instance\n",
        "\n",
        "    def __new__(cls, *args, **kwargs):\n",
        "        if not cls._instance:\n",
        "            cls._instance = super().__new__(cls, *args, **kwargs)\n",
        "            # Generate CKKS keys upon instantiation\n",
        "            cls._instance.__context = ts.context(\n",
        "                ts.SCHEME_TYPE.CKKS,\n",
        "                poly_modulus_degree=8192,\n",
        "                coeff_mod_bit_sizes=[60, 40, 40, 60]\n",
        "            )\n",
        "            cls._instance.__context.generate_galois_keys()\n",
        "            cls._instance.__context.global_scale = 2**40\n",
        "            cls._instance.__public_key = cls._instance.__context.public_key()\n",
        "            cls._instance.__relin_keys = cls._instance.__context.relin_keys()\n",
        "            cls._instance.__total_distr =  None\n",
        "            cls._instance.enc_norm_distr =  None\n",
        "            cls._instance.clients = []\n",
        "        return cls._instance\n",
        "\n",
        "\n",
        "\n",
        "    def encrypt_with_public_key(self, data):\n",
        "      return ts.ckks_vector(self.__context, data)\n",
        "\n",
        "    def restore_with_public_key(self, data):\n",
        "      return ts.ckks_vector_from(self.__context, data)\n",
        "\n",
        "\n",
        "    def decryption(self):\n",
        "      return Client.cumulative_encrypted_distribution.decrypt()\n",
        "\n",
        "\n",
        "    def decrypt(self, encrypted_data):\n",
        "      return encrypted_data.decrypt()\n",
        "\n",
        "\n",
        "    def global_imbalance(self):\n",
        "      total_distr = [round(x) for x in self.__total_distr]\n",
        "      return min(total_distr)/max(total_distr)\n",
        "\n",
        "\n",
        "    def norm_enc(self):\n",
        "      total_distr = [round(x) for x in self.__total_distr]\n",
        "      total_distribution = np.linalg.norm(total_distr)\n",
        "      normalized_distr = [x/total_distribution for x in total_distr]\n",
        "      return ts.ckks_vector(self.__context, normalized_distr)\n",
        "\n",
        "\n",
        "    def add_client(self, client):\n",
        "      self.clients.append(client)\n",
        "\n",
        "\n",
        "    def cos_sim(self):\n",
        "      data = []\n",
        "      client_similarities = np.array(data)\n",
        "      self.enc_norm_distr = self.norm_enc()\n",
        "      #print(self.enc_norm_distr.decrypt())\n",
        "      for client in self.clients:\n",
        "        temp = client.cos_sim_calc(self.enc_norm_distr)\n",
        "        temp_decrypt = temp.decrypt()\n",
        "        client_similarities = np.append(client_similarities, temp_decrypt)\n",
        "      return client_similarities\n",
        "\n",
        "\n",
        "\n",
        "    def similarity_comparison(self, enc_selected_vectors, calling_client):\n",
        "        results = []\n",
        "        for client in self.clients:\n",
        "            if client != calling_client:\n",
        "                result = client.plain_enc_mul(enc_selected_vectors)\n",
        "                results.append(result)\n",
        "        return results[0], results[1], results[2]\n",
        "\n",
        "\n",
        "    def balance_check(self):\n",
        "\n",
        "      self.__total_distr = self.decryption()\n",
        "      print(\"Initial Distr : \", self.__total_distr)\n",
        "      global_imbalance = self.global_imbalance()\n",
        "      print(\"Initial Imbal : \",  global_imbalance)\n",
        "      global_similarity_cl = self.cos_sim()\n",
        "      print(\"initial_global_similarity : \",  global_similarity_cl)\n",
        "      k = 1\n",
        "      GI_flag1, GI_flag2 = 0, 0\n",
        "      sorted_indices = np.argsort(global_similarity_cl)\n",
        "      round = 0\n",
        "      while(global_imbalance < 0.1):\n",
        "        selected_client_index = sorted_indices[-k]\n",
        "        selected_client = self.clients[selected_client_index]\n",
        "        print(\"selected_client : \",selected_client_index)\n",
        "        enc_results = selected_client.trigger(GI_flag1,GI_flag2, 0)\n",
        "        if(enc_results == 0):\n",
        "          k = k + 1\n",
        "          if(k==5):\n",
        "            k = 1\n",
        "            sorted_indices = np.argsort(global_similarity_cl)\n",
        "            selected_client_index = sorted_indices[-k]\n",
        "            selected_client = self.clients[selected_client_index]\n",
        "            if(selected_client.trigger(GI_flag1,GI_flag2, 0) == 0):\n",
        "              print(\"That's the best balance you can get\")\n",
        "              print(\"number of rounds  : \", round)\n",
        "              break\n",
        "          GI_flag1, GI_flag2 = 0, 0\n",
        "          continue\n",
        "        prev_global_imbalance = global_imbalance\n",
        "        self.__total_distr = enc_results.decrypt()\n",
        "        global_imbalance = self.global_imbalance()\n",
        "        global_similarity_cl = self.cos_sim()\n",
        "        print(\"global_imbalance : \", global_imbalance)\n",
        "        print(\"global_similarities : \", global_similarity_cl)\n",
        "        if(prev_global_imbalance >= global_imbalance):\n",
        "          GI_flag1 = 1\n",
        "        enc_results = selected_client.trigger(GI_flag1,GI_flag2, 1)\n",
        "        if(enc_results == 0):\n",
        "          k = k + 1\n",
        "          if(k==5):\n",
        "            k = 1\n",
        "            sorted_indices = np.argsort(global_similarity_cl)\n",
        "            selected_client_index = sorted_indices[-k]\n",
        "            selected_client = self.clients[selected_client_index]\n",
        "            if(selected_client.trigger(GI_flag1,GI_flag2, 0) == 0):\n",
        "              print(\"That's the best balance you can get\")\n",
        "              print(\"number of rounds  : \", round)\n",
        "              break\n",
        "          GI_flag1, GI_flag2 = 0, 0\n",
        "          continue\n",
        "\n",
        "        prev_global_imbalance = global_imbalance\n",
        "        self.__total_distr = enc_results.decrypt()\n",
        "        global_imbalance = self.global_imbalance()\n",
        "        global_similarity_cl = self.cos_sim()\n",
        "        print(\"global_imbalance : \", global_imbalance)\n",
        "        print(\"global_similarities : \", global_similarity_cl)\n",
        "        if(prev_global_imbalance >= global_imbalance):\n",
        "          GI_flag2 = 1\n",
        "        round = round + 1\n",
        "      print(\" Balancing Done in rounds = \", round)\n",
        "      return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Is5s-R46Sd1d"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "import random\n",
        "class Client:\n",
        "    cumulative_encrypted_distribution = None  # Class variable to store cumulative encrypted distribution\n",
        "    def __init__(self, distr, server, client_id):\n",
        "\n",
        "        if len(distr) != 4:\n",
        "            raise ValueError(\"class_samples must be a list or array of four integers.\")\n",
        "\n",
        "        self.context = ts.context(\n",
        "          ts.SCHEME_TYPE.CKKS,\n",
        "          poly_modulus_degree=8192,\n",
        "          coeff_mod_bit_sizes=[60, 40, 40, 60]\n",
        "        )\n",
        "        self.context.generate_galois_keys()\n",
        "        self.context.global_scale = 2**40\n",
        "\n",
        "\n",
        "        self.__distr = distr  # Private attribute\n",
        "        self.__dataset = self.__allocate_dataset(distr)  # Allocate and store the dataset\n",
        "        self.__feature_vectors = feature_extractor(self.__dataset[0])\n",
        "        self.__imbalance = min(self.__distr)/max(self.__distr)\n",
        "        self.client_id = client_id\n",
        "        self.server = server\n",
        "        self._method_called = False\n",
        "        # Initialize the variable that will retain its value between calls\n",
        "        self.i = None\n",
        "        self.j = None\n",
        "        self.server.add_client(self)\n",
        "        self.__update_cumulative_distribution()\n",
        "\n",
        "    def __allocate_dataset(self, distr):\n",
        "        return subset_cifar10(distr)\n",
        "\n",
        "    def cos_sim_calc(self,enc_distr):\n",
        "      total =  np.linalg.norm(self.__distr)\n",
        "      norm_distr = [x/total for x in self.__distr]\n",
        "      #print(norm_distr)\n",
        "      vec = enc_distr.mul(norm_distr)\n",
        "      return vec.sum()\n",
        "\n",
        "    def encrypt_server_pub_key(self):\n",
        "      total_distribution = sum(self.__distr)\n",
        "      normalized_distr = [x/total_distribution for x in self.__distr]\n",
        "      return self.server.encrypt_with_public_key(self.__distr), self.server.encrypt_with_public_key(normalized_distr)\n",
        "\n",
        "    def use_dataset(self):\n",
        "      x_subset, y_subset = self.__dataset\n",
        "      # Example: Print the shape of the dataset\n",
        "      print(\"Dataset Shape:\", x_subset.shape, y_subset.shape)\n",
        "      # Example: Return some information about the dataset\n",
        "      print(\"feature vector:\",self.__feature_vectors.shape)\n",
        "      return x_subset, y_subset, self.__distr\n",
        "\n",
        "\n",
        "    def __update_cumulative_distribution(self):\n",
        "      \"\"\"\n",
        "      Updates the cumulative encrypted distribution of all clients with the current client's distribution.\n",
        "      \"\"\"\n",
        "      if Client.cumulative_encrypted_distribution is None:\n",
        "          encrypted_distribution, _ = self.encrypt_server_pub_key()\n",
        "          Client.cumulative_encrypted_distribution = encrypted_distribution\n",
        "      else:\n",
        "          # Depiction of Second Layer of Encryption for the Cumulative distr in Transit\n",
        "          serialized_data = Client.cumulative_encrypted_distribution.serialize()          # Serialize the encrypted vector for further encryption\n",
        "          decimal_representation = list(serialized_data)                                  # Convert to a list of decimal byte values\n",
        "          encrypted_vector1 = ts.ckks_vector(self.context, decimal_representation)        # Encrpt with client's public Key\n",
        "          a = encrypted_vector1.decrypt(self.context.secret_key())                        # Decrypt with Client's secret key\n",
        "          a = [round(i) for i in a]                                                       # Round of to th nearset integer to overcome encryption errors\n",
        "          serialized_data_from_decimal = bytes(a)\n",
        "          encrypted_vector = self.server.restore_with_public_key(serialized_data_from_decimal)  # Converting back to the single encryption format\n",
        "          # Single encryption operations for the data at a client\n",
        "          Client.cumulative_encrypted_distribution = encrypted_vector\n",
        "          Client.cumulative_encrypted_distribution = Client.cumulative_encrypted_distribution.add_(self.__distr)\n",
        "\n",
        "    def plain_enc_mul(self, enc_vector):\n",
        "      # Perform element-wise multiplication with the client's encrypted vector\n",
        "      plain_matrix = np.array(self.__feature_vectors)\n",
        "      plain_matrix = np.transpose(plain_matrix)\n",
        "      results = []\n",
        "      for i in range(math.ceil(plain_matrix.shape[1]/3000)):\n",
        "        result =  enc_vector.matmul(plain_matrix[:,(i * 3000):min(((i+1)*3000),plain_matrix.shape[1])])\n",
        "        results.append(result)\n",
        "      return results\n",
        "\n",
        "\n",
        "\n",
        "    def under_sampling(self, selected_class, feature_vectors):\n",
        "\n",
        "      cosine_sim_matrix = cosine_similarity(feature_vectors)\n",
        "      n = cosine_sim_matrix.shape[0]\n",
        "      #print(n)\n",
        "\n",
        "      # Compute the mean and variance for each row\n",
        "      row_means = np.mean(cosine_sim_matrix, axis=1)\n",
        "      row_variances = np.var(cosine_sim_matrix, axis=1)\n",
        "\n",
        "      # Sort the rows based on mean values in descending order\n",
        "      sorted_indices = np.argsort(row_means)[::-1]\n",
        "\n",
        "      # Select the top 1/10 of the rows with the highest mean values\n",
        "      selected_indices = sorted_indices[:int(n/5)]\n",
        "\n",
        "      # Calculate the variance of the selected rows\n",
        "      selected_var = np.var(cosine_sim_matrix[selected_indices], axis=0)\n",
        "\n",
        "      # Iterate through the remaining rows and select additional rows one by one\n",
        "      for i in sorted_indices[int(n/5):]:\n",
        "          temp_indices = np.append(selected_indices, i)\n",
        "          temp_var = np.var(cosine_sim_matrix[temp_indices], axis=0)\n",
        "          # If variance is lower, add the row to the selection\n",
        "          if np.sum(temp_var) < np.sum(selected_var):\n",
        "              selected_var = temp_var\n",
        "              selected_indices = temp_indices\n",
        "          # Break if the desired number of rows is reached\n",
        "          if len(selected_indices) == int(n/5):\n",
        "              break\n",
        "      print(\"images selected : \",len(selected_indices))\n",
        "      samples_to_remove = []\n",
        "      # Return the indices of the selected rows\n",
        "      data =[]\n",
        "      for i in selected_indices:\n",
        "        similarities = np.array(data)\n",
        "        enc_sim_1, enc_sim_2, enc_sim_3 = server.similarity_comparison(ts.ckks_vector(self.context,feature_vectors[i]),self)\n",
        "        for j in range(len(enc_sim_1)):\n",
        "          temp = enc_sim_1[j].decrypt()\n",
        "          similarities = np.concatenate((similarities,temp))\n",
        "        for j in range(len(enc_sim_2)):\n",
        "          temp = enc_sim_2[j].decrypt()\n",
        "          similarities = np.concatenate((similarities,temp))\n",
        "        for j in range(len(enc_sim_3)):\n",
        "          temp = enc_sim_3[j].decrypt()\n",
        "          similarities = np.concatenate((similarities,temp))\n",
        "        c = np.sum(similarities > 0.98)\n",
        "        if(c >= 300):\n",
        "          samples_to_remove.append(i)\n",
        "      print(\"images to be removed : \",len(samples_to_remove),\" of class \",(selected_class))\n",
        "      # Convert lists to numpy arrays for easy indexing\n",
        "      x_train = np.array(self.__dataset[0])\n",
        "      y_train = np.array(self.__dataset[1])\n",
        "      f_vectors = np.array(self.__feature_vectors)\n",
        "\n",
        "      # Get indices of the samples belonging to the selected class\n",
        "      selected_class_indices = np.where(y_train == selected_class)[0]\n",
        "\n",
        "      # Translate samples_to_remove indices to indices in the original dataset\n",
        "      remove_indices = selected_class_indices[samples_to_remove]\n",
        "\n",
        "      # Create masks for removing samples\n",
        "      mask = np.ones(len(y_train), dtype=bool)\n",
        "      mask[remove_indices] = False\n",
        "\n",
        "      # Apply masks to x_train and y_train to remove selected samples\n",
        "      x_train = x_train[mask]\n",
        "      y_train = y_train[mask]\n",
        "      f_vectors = f_vectors[mask]\n",
        "      Client.cumulative_encrypted_distribution = Client.cumulative_encrypted_distribution.sub_(self.__distr)\n",
        "      # Update the class distribution\n",
        "      new_distr = np.copy(self.__distr)\n",
        "      new_distr[selected_class] -= len(remove_indices)\n",
        "      self.__distr = new_distr\n",
        "      my_list = list(self.__dataset)\n",
        "      my_list[0] = x_train\n",
        "      my_list[1] = y_train\n",
        "      self.__dataset = tuple(my_list)\n",
        "      self.__feature_vectors = f_vectors\n",
        "      Client.cumulative_encrypted_distribution = Client.cumulative_encrypted_distribution.add_(self.__distr)\n",
        "      return 0\n",
        "\n",
        "\n",
        "    def image_augmentation(self, images, save_dir, num_augmented_images):\n",
        "      prefix='test',\n",
        "      data_augment = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "      )\n",
        "\n",
        "      if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "      # Convert images to numpy arrays and stack them\n",
        "      image_arrays = [img_to_array(image) for image in images]\n",
        "      image_array_batch = np.stack(image_arrays)\n",
        "\n",
        "      # Reshape the batch array to add an extra dimension\n",
        "      image_array_batch = image_array_batch.reshape((len(images),) + image_array_batch.shape[1:])\n",
        "\n",
        "      total_images_to_generate = num_augmented_images\n",
        "      i = 0\n",
        "\n",
        "      for batch in data_augment.flow(image_array_batch, batch_size=len(images), save_to_dir=save_dir, save_prefix=prefix, save_format='jpg'):\n",
        "        i += len(batch)\n",
        "        if i >= total_images_to_generate:\n",
        "          break\n",
        "      return 0\n",
        "\n",
        "\n",
        "    def over_sampling_from_folder(self, class_label, multiple):\n",
        "      new_images = []\n",
        "      new_labels = []\n",
        "      image_folder = '/content/generated_images3'\n",
        "      existing_images = self.__dataset[0]\n",
        "      existing_labels = self.__dataset[1]\n",
        "      f_vectors = np.array(self.__feature_vectors)\n",
        "      # Get the indices of the existing images belonging to the specified class label\n",
        "      class_indices = np.where(existing_labels == class_label)[0]\n",
        "      images = existing_images[class_indices]\n",
        "      self.image_augmentation(images,image_folder ,multiple)\n",
        "      # Count the number of existing images for the specified class label\n",
        "      num_existing_images = len(class_indices)\n",
        "      print(num_existing_images, end = \" \")\n",
        "      # Calculate the number of images to add\n",
        "      num_images_to_add = multiple\n",
        "      # Collect all filenames in the folder\n",
        "      filenames = os.listdir(image_folder)\n",
        "      # Shuffle the filenames\n",
        "      random.shuffle(filenames)\n",
        "\n",
        "      # Iterate through the images in the folder\n",
        "      for filename in filenames:\n",
        "          filepath = os.path.join(image_folder, filename)\n",
        "          image = cv2.imread(filepath)\n",
        "          new_images.append(image)\n",
        "          new_labels.append(class_label)\n",
        "      # Convert lists to numpy arrays\n",
        "      new_images = np.array(new_images)\n",
        "      new_labels = np.array(new_labels)\n",
        "      new_feat_vec = feature_extractor(new_images)\n",
        "      Client.cumulative_encrypted_distribution = Client.cumulative_encrypted_distribution.sub_(self.__distr)\n",
        "      # Concatenate new images and labels with existing dataset\n",
        "      combined_images = np.concatenate((existing_images, new_images), axis=0)\n",
        "      combined_labels = np.concatenate((existing_labels, new_labels), axis=0)\n",
        "      combined_feat_vec = np.concatenate((f_vectors, new_feat_vec), axis=0)\n",
        "      # Update distribution\n",
        "      new_distribution = np.copy(self.__distr)\n",
        "      new_distribution[class_label] += len(new_images)\n",
        "      print(num_images_to_add,\" : images added successfully in class\",class_label)\n",
        "\n",
        "      self.__distr = new_distribution\n",
        "      my_list = list(self.__dataset)\n",
        "      my_list[0] = combined_images\n",
        "      my_list[1] = combined_labels\n",
        "      self.__dataset = tuple(my_list)\n",
        "      self.__feature_vectors = combined_feat_vec\n",
        "      Client.cumulative_encrypted_distribution = Client.cumulative_encrypted_distribution.add_(self.__distr)\n",
        "      for file in glob.glob(\"/content/generated_images3/*\"): os.remove(file)\n",
        "      return 0\n",
        "\n",
        "\n",
        "    def trigger(self, GI_flag1, GI_flag2, sampling):\n",
        "      self.__imbalance = min(self.__distr)/max(self.__distr)\n",
        "      print(self.__imbalance)\n",
        "      if(self.__imbalance >= 0.05):\n",
        "        print(\"Client \",self.client_id,\" is already balanced\")\n",
        "        return 0\n",
        "\n",
        "      if not self._method_called:\n",
        "        # If this is the first call, initialize the variable to 0\n",
        "        self.i = 1\n",
        "        self.j = 0\n",
        "        self._method_called = True\n",
        "\n",
        "\n",
        "      if(sampling == 0):\n",
        "        if(GI_flag1 == 1):\n",
        "          self.i += 1\n",
        "          if(self.i == 3):\n",
        "            self.i = 0\n",
        "        sorted_indices = np.argsort(self.__distr)\n",
        "        # Get index of the smallest value (first element in sorted order)\n",
        "        selected_class = sorted_indices[-self.i]\n",
        "        selected_class_indices = np.where(self.__dataset[1] == selected_class)[0]\n",
        "        #print(len(selected_class_indices))\n",
        "        selected_feature_vectors = [self.__feature_vectors[index] for index in selected_class_indices]\n",
        "        self.under_sampling(selected_class, selected_feature_vectors)\n",
        "        return Client.cumulative_encrypted_distribution\n",
        "\n",
        "      if(sampling == 1):\n",
        "        if(GI_flag2 == 1):\n",
        "          self.j += 1\n",
        "        sorted_indices = np.argsort(self.__distr)\n",
        "        # Get index of the smallest value (first element in sorted order)\n",
        "        selected_class = sorted_indices[0]\n",
        "        multiple = int(1/(4 * self.__imbalance))\n",
        "        self.over_sampling_from_folder(selected_class,multiple)\n",
        "        return Client.cumulative_encrypted_distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCU7FCvxK0Ev"
      },
      "source": [
        "Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be6g8lBhWWeI"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "server = Server()\n",
        "\n",
        "\"\"\"distr1 = [10, 500, 700, 4000]\n",
        "distr2 = [20, 700, 500, 3000]\n",
        "distr3 = [30, 400, 600, 3000]\n",
        "distr4 = [100, 50, 200, 10]\"\"\"\n",
        "\n",
        "\"\"\"distr1 = [10, 30, 700, 4000]\n",
        "distr2 = [20, 40, 500, 3000]\n",
        "distr3 = [30, 40, 600, 3000]\n",
        "distr4 = [50, 50, 200, 10]\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"distr1 = [10, 30, 3600, 4000]\n",
        "distr2 = [20, 40, 2700, 3000]\n",
        "distr3 = [30, 40, 2600, 3000]\n",
        "distr4 = [100, 50, 200, 10]\"\"\"\n",
        "\n",
        "distr1 = [2, 100, 600, 4000]\n",
        "distr2 = [3, 200, 700, 3000]\n",
        "distr3 = [5, 150, 800, 3000]\n",
        "distr4 = [30, 50, 20, 10]\n",
        "\n",
        "client_1 = Client(distr1, server, 0)\n",
        "client_2 = Client(distr2, server, 1)\n",
        "client_3 = Client(distr3, server, 2)\n",
        "client_4 = Client(distr4, server, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9mUGPz-K9ua"
      },
      "source": [
        "Imblance not addressed at all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDo7zbF0K2_A"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvEpHmUTWhBG"
      },
      "outputs": [],
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "X = 0\n",
        "x_t, y_t, a = client_1.use_dataset()\n",
        "X+= len(y_t)\n",
        "x_train.append(x_t)\n",
        "y_train.append(y_t)\n",
        "x_t, y_t, b = client_2.use_dataset()\n",
        "X+= len(y_t)\n",
        "x_train.append(x_t)\n",
        "y_train.append(y_t)\n",
        "x_t, y_t, c = client_3.use_dataset()\n",
        "X+= len(y_t)\n",
        "x_train.append(x_t)\n",
        "y_train.append(y_t)\n",
        "x_t, y_t, d = client_4.use_dataset()\n",
        "X+= len(y_t)\n",
        "x_train.append(x_t)\n",
        "y_train.append(y_t)\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "print(d)\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeU855NxWtxA"
      },
      "outputs": [],
      "source": [
        "# Function to update the client's local model\n",
        "def client_update(local_model, img, label):\n",
        "    # Train the local model on client data\n",
        "    local_model.fit(img, label, epochs=4, batch_size = 64, verbose=0)  # You can adjust the number of epochs\n",
        "    # Return the updated local model\n",
        "    return local_model\n",
        "\n",
        "# Function to update the global model on the server\n",
        "def server_update(local_models):\n",
        "    local_weights = [model.get_weights() for model in local_models]\n",
        "    # Average the weights from all local models\n",
        "    averaged_weights = [sum(weights) / len(local_models) for weights in zip(*local_weights)]\n",
        "    # Update the global model with the averaged weights\n",
        "    updated_global_model = define_cifar10_model()\n",
        "    updated_global_model.set_weights(averaged_weights)\n",
        "    return updated_global_model\n",
        "\n",
        "# Function to evaluate the global model\n",
        "def evaluate(global_model, img, label):\n",
        "\n",
        "    _, accuracy = global_model.evaluate(img, label, verbose=0)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "#for plot\n",
        "rounds = []\n",
        "accuracies = []\n",
        "\n",
        "# Load the datasets\n",
        "_, _, x_test, y_test = load_dataset()\n",
        "x_train_A, x_train_B = prep_pixels(x_train[0], x_train[1])\n",
        "x_train_C, x_train_D = prep_pixels(x_train[2], x_train[3])\n",
        "x_test, x_test = prep_pixels(x_test,x_test)\n",
        "\n",
        "\n",
        "# Initialize the models for clients 0-4 and 5-9\n",
        "initial_model_A = create_model_with_initial_weights()\n",
        "initial_model_B = create_model_with_initial_weights()\n",
        "initial_model_C = create_model_with_initial_weights()\n",
        "initial_model_D = create_model_with_initial_weights()\n",
        "global_model = create_model_with_initial_weights()\n",
        "\n",
        "\n",
        "y_train_A = to_categorical(y_train[0])\n",
        "y_train_B = to_categorical(y_train[1])\n",
        "y_train_C = to_categorical(y_train[2])\n",
        "y_train_D = to_categorical(y_train[3])\n",
        "#y_test = to_categorical(y_test)\n",
        "global_model.summary()\n",
        "# federated learning\n",
        "num_rounds = 20\n",
        "\n",
        "for round_num in range(num_rounds):\n",
        "    accuracy = evaluate(global_model,x_test, y_test)\n",
        "    print(f\"Round {round_num}: Accuracy = {accuracy * 100}\")\n",
        "\n",
        "    global_weights = global_model.get_weights()\n",
        "    initial_model_A.set_weights(global_weights)\n",
        "    initial_model_B.set_weights(global_weights)\n",
        "    initial_model_C.set_weights(global_weights)\n",
        "    initial_model_D.set_weights(global_weights)\n",
        "    initial_model_A = client_update(initial_model_A, x_train_A, y_train_A)\n",
        "    initial_model_B = client_update(initial_model_B, x_train_B, y_train_B)\n",
        "    initial_model_C = client_update(initial_model_C, x_train_C, y_train_C)\n",
        "    initial_model_D = client_update(initial_model_D, x_train_D, y_train_D)\n",
        "    # Aggregate model updates on the server\n",
        "    global_model = server_update([initial_model_A, initial_model_B, initial_model_C, initial_model_D])\n",
        "    rounds.append(round_num)  # Add the current round number\n",
        "    accuracies.append(accuracy * 100)  # Add the accuracy for the current round\n",
        "\n",
        "accuracy = evaluate(global_model,x_test, y_test)\n",
        "print(f\"Round {round_num +1}: Accuracy = {accuracy * 100}\")\n",
        "# Evaluate the global model\n",
        "probabilities = global_model.predict(x_test)\n",
        "predicted_labels = np.argmax(probabilities, axis=1)\n",
        "predicted_labels_onehot = to_categorical(predicted_labels)\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, predicted_labels_onehot)\n",
        "print(report)\n",
        "\n",
        "\n",
        "\n",
        "rounds.append(round_num + 1)  # Add the current round number\n",
        "accuracies.append(accuracy * 100)  # Add the accuracy for the current round\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(rounds, accuracies, marker='o', label='Federated Model ')\n",
        "plt.title(\"Round vs Accuracy\")\n",
        "plt.xlabel(\"Round Number\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.grid(True)\n",
        "plt.legend()  # Add a legend to differentiate the lines\n",
        "plt.ylim(0, 100)\n",
        "plt.show()\n",
        "\n",
        "np_accuracy = np.array(accuracies)\n",
        "np.savetxt('accuracy.txt', np_accuracy, fmt='%f', delimiter=',')\n",
        "global_model.save('no_bal.keras')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3vwIDJZHVjG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Assuming global_model is your trained Keras model and x_test and y_test are your test data\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_prob = global_model.predict(x_test)\n",
        "\n",
        "# Assuming y_test is one-hot encoded\n",
        "n_classes = y_test.shape[1]\n",
        "y_test_binary = y_test\n",
        "\n",
        "# Calculate ROC curve and AUC for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_binary[:, i], y_pred_prob[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "plt.figure()\n",
        "colors = ['aqua', 'darkorange', 'cornflowerblue', 'green']  # Adjust the number of colors based on the number of classes\n",
        "for i in range(n_classes):\n",
        "    plt.plot(fpr[i], tpr[i], color=colors[i % len(colors)], lw=2,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic for Multiclass')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Print AUC for each class\n",
        "for i in range(n_classes):\n",
        "    print(f'AUC for class {i}: {roc_auc[i]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErPNmWN9LFR2"
      },
      "source": [
        "Base-Line-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwzmsdOYIQf3"
      },
      "outputs": [],
      "source": [
        "x_train[0], y_train[0], LD_0 = base_under_sampling(x_train[0], y_train[0], distr1)\n",
        "x_train[1], y_train[1], LD_1 = base_under_sampling(x_train[1], y_train[1], distr2)\n",
        "x_train[2], y_train[2], LD_2 = base_under_sampling(x_train[2], y_train[2], distr3)\n",
        "x_train[3], y_train[3], LD_3 = base_under_sampling(x_train[3], y_train[3], distr4)\n",
        "y = LD_0 + LD_1 + LD_2 + LD_3\n",
        "Y = np.sum(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv0_QSvfbKiS"
      },
      "outputs": [],
      "source": [
        "print(LD_0)\n",
        "print(LD_1)\n",
        "print(LD_2)\n",
        "print(LD_3)\n",
        "print(Y)\n",
        "print()\n",
        "print(distr1)\n",
        "print(distr2)\n",
        "print(distr3)\n",
        "print(distr4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHa5PawWaLUt"
      },
      "outputs": [],
      "source": [
        "# Function to update the client's local model\n",
        "def client_update(local_model, img, label):\n",
        "    # Train the local model on client data\n",
        "    local_model.fit(img, label, epochs=4, batch_size = 64, verbose=0)  # You can adjust the number of epochs\n",
        "    # Return the updated local model\n",
        "    return local_model\n",
        "\n",
        "# Function to update the global model on the server\n",
        "def server_update(local_models):\n",
        "    local_weights = [model.get_weights() for model in local_models]\n",
        "    # Average the weights from all local models\n",
        "    averaged_weights = [sum(weights) / len(local_models) for weights in zip(*local_weights)]\n",
        "    # Update the global model with the averaged weights\n",
        "    updated_global_model = define_cifar10_model()\n",
        "    updated_global_model.set_weights(averaged_weights)\n",
        "    return updated_global_model\n",
        "\n",
        "# Function to evaluate the global model\n",
        "def evaluate(global_model, img, label):\n",
        "\n",
        "    _, accuracy = global_model.evaluate(img, label, verbose=0)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "#for plot\n",
        "rounds1 = []\n",
        "accuracies1 = []\n",
        "\n",
        "# Load the datasets\n",
        "_, _, x_test, y_test = load_dataset()\n",
        "x_train_A, x_train_B = prep_pixels(x_train[0], x_train[1])\n",
        "x_train_C, x_train_D = prep_pixels(x_train[2], x_train[3])\n",
        "x_test, _ = prep_pixels(x_test,_)\n",
        "\n",
        "\n",
        "# Initialize the models for clients 0-4 and 5-9\n",
        "initial_model_A = create_model_with_initial_weights()\n",
        "initial_model_B = create_model_with_initial_weights()\n",
        "initial_model_C = create_model_with_initial_weights()\n",
        "initial_model_D = create_model_with_initial_weights()\n",
        "global_model = create_model_with_initial_weights()\n",
        "\n",
        "y_train_A = to_categorical(y_train[0])\n",
        "y_train_B = to_categorical(y_train[1])\n",
        "y_train_C = to_categorical(y_train[2])\n",
        "y_train_D = to_categorical(y_train[3])\n",
        "#y_test = to_categorical(y_test)\n",
        "global_model.summary()\n",
        "# federated learning\n",
        "num_rounds = 20\n",
        "denom = Y/X\n",
        "for round_num in range(num_rounds):\n",
        "    accuracy = evaluate(global_model,x_test, y_test)\n",
        "\n",
        "    accuracy = accuracy/max(1,denom)\n",
        "    print(f\"Round {round_num}: Accuracy = {accuracy * 100}\")\n",
        "\n",
        "    global_weights = global_model.get_weights()\n",
        "    initial_model_A.set_weights(global_weights)\n",
        "    initial_model_B.set_weights(global_weights)\n",
        "    initial_model_C.set_weights(global_weights)\n",
        "    initial_model_D.set_weights(global_weights)\n",
        "    initial_model_A = client_update(initial_model_A, x_train_A, y_train_A)\n",
        "    initial_model_B = client_update(initial_model_B, x_train_B, y_train_B)\n",
        "    initial_model_C = client_update(initial_model_C, x_train_C, y_train_C)\n",
        "    initial_model_D = client_update(initial_model_D, x_train_D, y_train_D)\n",
        "    # Aggregate model updates on the server\n",
        "    global_model = server_update([initial_model_A, initial_model_B, initial_model_C, initial_model_D])\n",
        "    rounds1.append(round_num)  # Add the current round number\n",
        "    accuracies1.append(accuracy * 100)  # Add the accuracy for the current round\n",
        "\n",
        "accuracy = evaluate(global_model,x_test, y_test)\n",
        "accuracy = accuracy/max(1,denom)\n",
        "print(f\"Round {round_num +1}: Accuracy = {accuracy * 100}\")\n",
        "# Evaluate the global model\n",
        "probabilities = global_model.predict(x_test)\n",
        "predicted_labels = np.argmax(probabilities, axis=1)\n",
        "predicted_labels_onehot = to_categorical(predicted_labels)\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, predicted_labels_onehot)\n",
        "print(report)\n",
        "\n",
        "\n",
        "\n",
        "rounds1.append(round_num + 1)  # Add the current round number\n",
        "accuracies1.append(accuracy * 100)  # Add the accuracy for the current round\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(rounds1, accuracies1, marker='o', label='Federated Model')\n",
        "plt.title(\"Round vs Accuracy\")\n",
        "plt.xlabel(\"Round Number\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.grid(True)\n",
        "plt.legend()  # Add a legend to differentiate the lines\n",
        "plt.ylim(0, 100)\n",
        "plt.show()\n",
        "np_accuracy1 = np.array(accuracies1)\n",
        "np.savetxt('accuracy1.txt', np_accuracy1, fmt='%f', delimiter=',')\n",
        "global_model.save('US_bal.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIm3PoNjJPNn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "\n",
        "# Assuming global_model is your trained Keras model and x_test and y_test are your test data\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_prob = global_model.predict(x_test)\n",
        "\n",
        "# Assuming y_test is one-hot encoded\n",
        "n_classes = y_test.shape[1]\n",
        "y_test_binary = y_test\n",
        "\n",
        "# Calculate ROC curve and AUC for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_binary[:, i], y_pred_prob[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "plt.figure()\n",
        "colors = ['aqua', 'darkorange', 'cornflowerblue', 'green']  # Adjust the number of colors based on the number of classes\n",
        "for i in range(n_classes):\n",
        "    plt.plot(fpr[i], tpr[i], color=colors[i % len(colors)], lw=2,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic for Multiclass')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Print AUC for each class\n",
        "for i in range(n_classes):\n",
        "    print(f'AUC for class {i}: {roc_auc[i]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo1m2imfa922"
      },
      "source": [
        "Baseline-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W40oPcA_atgs"
      },
      "outputs": [],
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "x_t, y_t, a = client_1.use_dataset()\n",
        "x_train.append(x_t)\n",
        "y_train.append(y_t)\n",
        "x_t, y_t, b = client_2.use_dataset()\n",
        "x_train.append(x_t)\n",
        "y_train.append(y_t)\n",
        "x_t, y_t, c = client_3.use_dataset()\n",
        "x_train.append(x_t)\n",
        "y_train.append(y_t)\n",
        "x_t, y_t, d = client_4.use_dataset()\n",
        "x_train.append(x_t)\n",
        "y_train.append(y_t)\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3BJdgfFg308"
      },
      "outputs": [],
      "source": [
        "x_train[0], y_train[0], LD_0 = over_sampling_from_folder_clien(x_train[0], y_train[0], distr1)\n",
        "x_train[1], y_train[1], LD_1 = over_sampling_from_folder_clien(x_train[1], y_train[1], distr2)\n",
        "x_train[2], y_train[2], LD_2 = over_sampling_from_folder_clien(x_train[2], y_train[2], distr3)\n",
        "x_train[3], y_train[3], LD_3 = over_sampling_from_folder_clien(x_train[3], y_train[3], distr4)\n",
        "y = LD_0 + LD_1 + LD_2 + LD_3\n",
        "Y = np.sum(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE2a9eSFjV1X"
      },
      "outputs": [],
      "source": [
        "print(LD_0)\n",
        "print(LD_1)\n",
        "print(LD_2)\n",
        "print(LD_3)\n",
        "print(Y)\n",
        "print()\n",
        "print(distr1)\n",
        "print(distr2)\n",
        "print(distr3)\n",
        "print(distr4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9wCaGvjIg2s"
      },
      "outputs": [],
      "source": [
        "# Function to update the client's local model\n",
        "def client_update(local_model, img, label):\n",
        "    # Train the local model on client data\n",
        "    local_model.fit(img, label, epochs=4, batch_size = 64, verbose=0)  # You can adjust the number of epochs\n",
        "    # Return the updated local model\n",
        "    return local_model\n",
        "\n",
        "# Function to update the global model on the server\n",
        "def server_update(local_models):\n",
        "    local_weights = [model.get_weights() for model in local_models]\n",
        "    # Average the weights from all local models\n",
        "    averaged_weights = [sum(weights) / len(local_models) for weights in zip(*local_weights)]\n",
        "    # Update the global model with the averaged weights\n",
        "    updated_global_model = define_cifar10_model()\n",
        "    updated_global_model.set_weights(averaged_weights)\n",
        "    return updated_global_model\n",
        "\n",
        "# Function to evaluate the global model\n",
        "def evaluate(global_model, img, label):\n",
        "\n",
        "    _, accuracy = global_model.evaluate(img, label, verbose=0)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "#for plot\n",
        "rounds2 = []\n",
        "accuracies2 = []\n",
        "\n",
        "# Load the datasets\n",
        "_, _, x_test, y_test = load_dataset()\n",
        "x_train_A, x_train_B = prep_pixels(x_train[0], x_train[1])\n",
        "x_train_C, x_train_D = prep_pixels(x_train[2], x_train[3])\n",
        "x_test, _ = prep_pixels(x_test,_)\n",
        "\n",
        "\n",
        "# Initialize the models for clients 0-4 and 5-9\n",
        "initial_model_A = create_model_with_initial_weights()\n",
        "initial_model_B = create_model_with_initial_weights()\n",
        "initial_model_C = create_model_with_initial_weights()\n",
        "initial_model_D = create_model_with_initial_weights()\n",
        "global_model = create_model_with_initial_weights()\n",
        "\n",
        "y_train_A = to_categorical(y_train[0])\n",
        "y_train_B = to_categorical(y_train[1])\n",
        "y_train_C = to_categorical(y_train[2])\n",
        "y_train_D = to_categorical(y_train[3])\n",
        "#y_test = to_categorical(y_test)\n",
        "global_model.summary()\n",
        "# federated learning\n",
        "num_rounds = 20\n",
        "deno = Y/X\n",
        "\n",
        "\n",
        "\n",
        "for round_num in range(num_rounds):\n",
        "    accuracy = evaluate(global_model,x_test, y_test)\n",
        "    accuracy = accuracy/max(1,deno)\n",
        "    print(f\"Round {round_num}: Accuracy = {accuracy * 100}\")\n",
        "\n",
        "    global_weights = global_model.get_weights()\n",
        "    initial_model_A.set_weights(global_weights)\n",
        "    initial_model_B.set_weights(global_weights)\n",
        "    initial_model_C.set_weights(global_weights)\n",
        "    initial_model_D.set_weights(global_weights)\n",
        "    initial_model_A = client_update(initial_model_A, x_train_A, y_train_A)\n",
        "    initial_model_B = client_update(initial_model_B, x_train_B, y_train_B)\n",
        "    initial_model_C = client_update(initial_model_C, x_train_C, y_train_C)\n",
        "    initial_model_D = client_update(initial_model_D, x_train_D, y_train_D)\n",
        "    # Aggregate model updates on the server\n",
        "    global_model = server_update([initial_model_A, initial_model_B, initial_model_C, initial_model_D])\n",
        "    rounds2.append(round_num)  # Add the current round number\n",
        "    accuracies2.append(accuracy * 100)  # Add the accuracy for the current round\n",
        "\n",
        "accuracy = evaluate(global_model,x_test, y_test)\n",
        "accuracy = accuracy/max(1,deno)\n",
        "print(f\"Round {round_num +1}: Accuracy = {accuracy * 100}\")\n",
        "# Evaluate the global model\n",
        "probabilities = global_model.predict(x_test)\n",
        "predicted_labels = np.argmax(probabilities, axis=1)\n",
        "predicted_labels_onehot = to_categorical(predicted_labels)\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, predicted_labels_onehot)\n",
        "print(report)\n",
        "\n",
        "\n",
        "\n",
        "rounds2.append(round_num + 1)  # Add the current round number\n",
        "accuracies2.append(accuracy * 100)  # Add the accuracy for the current round\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(rounds2, accuracies2, marker='o', label='Federated Model')\n",
        "plt.title(\"Round vs Accuracy\")\n",
        "plt.xlabel(\"Round Number\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.grid(True)\n",
        "plt.legend()  # Add a legend to differentiate the lines\n",
        "plt.ylim(0, 100)\n",
        "plt.show()\n",
        "np_accuracy2 = np.array(accuracies2)\n",
        "np.savetxt('accuracy2.txt', np_accuracy2, fmt='%f', delimiter=',')\n",
        "global_model.save('OS_bal.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzdXTFzEJZhi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "\n",
        "# Assuming global_model is your trained Keras model and x_test and y_test are your test data\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_prob = global_model.predict(x_test)\n",
        "\n",
        "# Assuming y_test is one-hot encoded\n",
        "n_classes = y_test.shape[1]\n",
        "y_test_binary = y_test\n",
        "\n",
        "# Calculate ROC curve and AUC for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_binary[:, i], y_pred_prob[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "plt.figure()\n",
        "colors = ['aqua', 'darkorange', 'cornflowerblue', 'green']  # Adjust the number of colors based on the number of classes\n",
        "for i in range(n_classes):\n",
        "    plt.plot(fpr[i], tpr[i], color=colors[i % len(colors)], lw=2,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic for Multiclass')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Print AUC for each class\n",
        "for i in range(n_classes):\n",
        "    print(f'AUC for class {i}: {roc_auc[i]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1TypfvsLLOf"
      },
      "source": [
        "Recommended Scheme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PP8HZPV1JSvv"
      },
      "outputs": [],
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "x_t, y_t, a = client_1.use_dataset()\n",
        "x_train.append(x_t)\n",
        "y_train.append(y_t)\n",
        "x_t, y_t, b = client_2.use_dataset()\n",
        "x_train.append(x_t)\n",
        "y_train.append(y_t)\n",
        "x_t, y_t, c = client_3.use_dataset()\n",
        "x_train.append(x_t)\n",
        "y_train.append(y_t)\n",
        "x_t, y_t, d = client_4.use_dataset()\n",
        "x_train.append(x_t)\n",
        "y_train.append(y_t)\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqwkJJzcWj_M"
      },
      "outputs": [],
      "source": [
        "server.balance_check()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uodusezI1Cq5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmQeyG_VW0Pj"
      },
      "outputs": [],
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "x_t, y_t, a = client_1.use_dataset()\n",
        "x_train.append(x_t)\n",
        "y_train.append(y_t)\n",
        "x_t, y_t, b = client_2.use_dataset()\n",
        "x_train.append(x_t)\n",
        "y_train.append(y_t)\n",
        "x_t, y_t, c = client_3.use_dataset()\n",
        "x_train.append(x_t)\n",
        "y_train.append(y_t)\n",
        "x_t, y_t, d = client_4.use_dataset()\n",
        "x_train.append(x_t)\n",
        "y_train.append(y_t)\n",
        "y = a + b + c + d\n",
        "Y = np.sum(y)\n",
        "\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "print(d)\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVqgYA4WXFYf"
      },
      "outputs": [],
      "source": [
        "# Function to update the client's local model\n",
        "def client_update(local_model, img, label):\n",
        "    # Train the local model on client data\n",
        "    local_model.fit(img, label, epochs=4, batch_size = 64, verbose=0)  # You can adjust the number of epochs\n",
        "    # Return the updated local model\n",
        "    return local_model\n",
        "\n",
        "# Function to update the global model on the server\n",
        "def server_update(local_models):\n",
        "    local_weights = [model.get_weights() for model in local_models]\n",
        "    # Average the weights from all local models\n",
        "    averaged_weights = [sum(weights) / len(local_models) for weights in zip(*local_weights)]\n",
        "    # Update the global model with the averaged weights\n",
        "    updated_global_model = define_cifar10_model()\n",
        "    updated_global_model.set_weights(averaged_weights)\n",
        "    return updated_global_model\n",
        "\n",
        "# Function to evaluate the global model\n",
        "def evaluate(global_model, img, label):\n",
        "\n",
        "    _, accuracy = global_model.evaluate(img, label, verbose=0)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "#for plot\n",
        "rounds3 = []\n",
        "accuracies3 = []\n",
        "\n",
        "# Load the datasets\n",
        "_, _, x_test, y_test = load_dataset()\n",
        "x_train_A, x_train_B = prep_pixels(x_train[0], x_train[1])\n",
        "x_train_C, x_train_D = prep_pixels(x_train[2], x_train[3])\n",
        "x_test, _ = prep_pixels(x_test,_)\n",
        "\n",
        "\n",
        "# Initialize the models for clients 0-4 and 5-9\n",
        "initial_model_A = create_model_with_initial_weights()\n",
        "initial_model_B = create_model_with_initial_weights()\n",
        "initial_model_C = create_model_with_initial_weights()\n",
        "initial_model_D = create_model_with_initial_weights()\n",
        "global_model = create_model_with_initial_weights()\n",
        "\n",
        "y_train_A = to_categorical(y_train[0])\n",
        "y_train_B = to_categorical(y_train[1])\n",
        "y_train_C = to_categorical(y_train[2])\n",
        "y_train_D = to_categorical(y_train[3])\n",
        "#y_test = to_categorical(y_test)\n",
        "global_model.summary()\n",
        "# federated learning\n",
        "num_rounds = 20\n",
        "denom = Y/X\n",
        "for round_num in range(num_rounds):\n",
        "    accuracy = evaluate(global_model,x_test, y_test)\n",
        "    accuracy = accuracy/max(1,denom)\n",
        "    print(f\"Round {round_num}: Accuracy = {accuracy * 100}\")\n",
        "    global_weights = global_model.get_weights()\n",
        "    initial_model_A.set_weights(global_weights)\n",
        "    initial_model_B.set_weights(global_weights)\n",
        "    initial_model_C.set_weights(global_weights)\n",
        "    initial_model_D.set_weights(global_weights)\n",
        "    initial_model_A = client_update(initial_model_A, x_train_A, y_train_A)\n",
        "    initial_model_B = client_update(initial_model_B, x_train_B, y_train_B)\n",
        "    initial_model_C = client_update(initial_model_C, x_train_C, y_train_C)\n",
        "    initial_model_D = client_update(initial_model_D, x_train_D, y_train_D)\n",
        "    # Aggregate model updates on the server\n",
        "    global_model = server_update([initial_model_A, initial_model_B, initial_model_C, initial_model_D])\n",
        "    rounds3.append(round_num)  # Add the current round number\n",
        "    accuracies3.append(accuracy * 100)  # Add the accuracy for the current round\n",
        "\n",
        "accuracy = evaluate(global_model,x_test, y_test)\n",
        "accuracy = accuracy/max(1,denom)\n",
        "print(f\"Round {round_num +1}: Accuracy = {accuracy * 100}\")\n",
        "# Evaluate the global model\n",
        "probabilities = global_model.predict(x_test)\n",
        "predicted_labels = np.argmax(probabilities, axis=1)\n",
        "predicted_labels_onehot = to_categorical(predicted_labels)\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, predicted_labels_onehot)\n",
        "print(report)\n",
        "\n",
        "\n",
        "\n",
        "rounds3.append(round_num + 1)  # Add the current round number\n",
        "accuracies3.append(accuracy * 100)  # Add the accuracy for the current round\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(rounds3, accuracies3, marker='o', label='Federated Model')\n",
        "plt.title(\"Round vs Accuracy\")\n",
        "plt.xlabel(\"Round Number\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.grid(True)\n",
        "plt.legend()  # Add a legend to differentiate the lines\n",
        "plt.ylim(0, 100)\n",
        "plt.show()\n",
        "\n",
        "np_accuracy3 = np.array(accuracies3)\n",
        "np.savetxt('accuracy3.txt', np_accuracy3, fmt='%f', delimiter=',')\n",
        "global_model.save('proposed_bal.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ic7eA_9Jdgc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "\n",
        "# Assuming global_model is your trained Keras model and x_test and y_test are your test data\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_prob = global_model.predict(x_test)\n",
        "\n",
        "# Assuming y_test is one-hot encoded\n",
        "n_classes = y_test.shape[1]\n",
        "y_test_binary = y_test\n",
        "\n",
        "# Calculate ROC curve and AUC for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_binary[:, i], y_pred_prob[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "plt.figure()\n",
        "colors = ['aqua', 'darkorange', 'cornflowerblue', 'green']  # Adjust the number of colors based on the number of classes\n",
        "for i in range(n_classes):\n",
        "    plt.plot(fpr[i], tpr[i], color=colors[i % len(colors)], lw=2,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic for Multiclass')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Print AUC for each class\n",
        "for i in range(n_classes):\n",
        "    print(f'AUC for class {i}: {roc_auc[i]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3mzBRA9XI7I"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(rounds, accuracies, marker='o', markersize=8, linewidth=1, label='Training with Class Imbalance')\n",
        "plt.plot(rounds1, accuracies1, marker='h', markersize=8, linewidth=1, label='Under Sampling')\n",
        "plt.plot(rounds2, accuracies2, marker='s', markersize=8, linewidth=1, label='Over Sampling')\n",
        "plt.plot(rounds3, accuracies3, marker='>', markersize=8, linewidth=1, label='FLICKER')\n",
        "\n",
        "plt.title(\"Round vs Accuracy\", fontsize=16, fontweight='bold')\n",
        "plt.xlabel(\"Round Number\", fontsize=14, fontweight='bold')\n",
        "plt.ylabel(\"Normalized Accuracy (%)\", fontsize=14, fontweight='bold')\n",
        "plt.grid(True)\n",
        "plt.legend(fontsize=14)  # Add a legend to differentiate the lines\n",
        "plt.ylim(20, 80)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}